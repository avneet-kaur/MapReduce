# IMPLEMENTATION 
The problem statement is to count number of words in any dataset and avoid stop words. So, to skip stop words, Java HashSet is created for list of words which are further getting bypassed In Mapper function.

# Mapper Phase : (MyMapper.java)
The first phase of processing data that processes each input record and generates <key,value> pair. The StringTokenizer tokenize the text file. The StringTokenizer is being iterated and during iteration, words which are not a stop word they are passed to next phase: Combiner or Reducer with value one.
In code, it is implemented as MyMapper.java which extends the Mapper class from org.apache.hadoop.mapreduce.Mapper.

    Number of mappers = number of blocks (Input file is divided into blocks)
    Eg :- Hello, 1
    
## Shuffle Phase
The process by which system performs sort and transfer the map output to reducer input. 
    
    The output is in the form: <key,list(Values)>
    Eg :-Hello,[1,1]

## Sort Phase
Sorting the output from different mappers on the basis of key generated by different Mappers.

# Reducer Phase: (MyReducer.java)
This phase summarize operation. By default there is one reducer which are configurable. The output of reducer is the final output which is stored in DFS. Reducers run in parallel because they are independent of one another. In reducer, for each key, the summation of iterable list of values is done to check the frequency of that key.
In code, it is implemented as MyReducer.java which extends the Reducer class from org.apache.hadoop.mapreduce.Reducer .

      Eg:- Hello, 2

# Procedure to execute:-

1. Start daemon DataNode and NameNode.
2. Browse web interface for NameNode :- http://localhost:9870/
3. Create input directory on hdfs:

        $ hadoop fs -mkdir /input_dir
    
4. Upload input file in DFS:

        $ hadoop fs -put /Users/Avneetpannu/Documents/assignment5/count.txt /inputDir.
5. Build the jar file.
6. Run: 
        
        $ hadoop jar Counter.jar /input_dir /output_directory
       
     Eg :- hadoop jar /Users/Avneetpannu/Documents/assignment5/verify.jar WordCountMapReducer /input_dir /output_dir
        
7. Check the output file from DFS browser. OR Check the output file from terminal output_dir.

        $ hadoop fs -ls output_dir
        
        $ hadoop fs -cat output_dir/part-r-00000
        
        $ hdfs dfs -cat /output_StopWords/*
